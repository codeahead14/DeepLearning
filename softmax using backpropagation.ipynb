{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Owing to KxN structure of the dot product, bias should be of the shape Kx1\n",
    "def softmax(features,coefficients,bias,include_bias=0):\n",
    "    ## Here the shape of the dot product is tranposed so as to allow efficient subtraction    \n",
    "    dp = np.dot(features,coefficients).transpose().astype('float32') + include_bias*bias\n",
    "    const = np.max(dp,axis=0)\n",
    "    return np.exp(dp-const)/np.sum(np.exp(dp-const),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(features,coefficients,labels,step_size=0.001,max_iter=1000):\n",
    "    coeff_dim = np.shape(coefficients)\n",
    "    features_dim = np.shape(features)\n",
    "    bias = np.ones((1,coeff_dim[1]))\n",
    "    features_bias = np.ones((features_dim[0],1))\n",
    "    print np.shape(features_bias)\n",
    "    print features_dim\n",
    "    \n",
    "    coefficients_with_bias = np.vstack((bias,coefficients))\n",
    "    features_with_bias = np.hstack((features_bias,features))\n",
    "    print features_with_bias\n",
    "    coeff_dim = np.shape(coefficients_with_bias)\n",
    "    features_dim = np.shape(features_bias)\n",
    "        \n",
    "    for _ in range(1000):\n",
    "        ## softmax probabilities - shape (KxN)\n",
    "        probs = softmax(features_with_bias,coefficients_with_bias,bias.T)\n",
    "\n",
    "        ## backpropagation\n",
    "        probs[labels,range(features_dim[0])] = probs[labels,range(features_dim[0])] - 1\n",
    "        coefficients_with_bias = coefficients_with_bias - step_size*(np.dot(features_with_bias.T,probs.T))\n",
    "    \n",
    "    return coefficients_with_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.03293347e-06   7.57565233e-10]\n",
      " [  1.21174700e-04   1.52161022e-08]\n",
      " [  1.79839209e-02   9.11051116e-04]\n",
      " [  9.81888831e-01   9.99088883e-01]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3,4],[4,5,6,7],[7,7,9,10]])\n",
    "y = np.array([[1,2,1],[2,1,4]])\n",
    "b = np.ones((4,1))\n",
    "z = softmax(y,x,b)\n",
    "print z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2L, 1L)\n",
      "(2L, 3L)\n",
      "[[ 1.  1.  2.  1.]\n",
      " [ 1.  2.  1.  4.]]\n",
      "[[ 1.91480233  1.27502044  0.64658281  0.16359442]\n",
      " [ 1.91212418  2.78208909  2.50479708  2.80098965]\n",
      " [ 5.83228281  5.04297223  5.43495135  5.68979361]\n",
      " [ 7.90676788  8.79622639  8.22122561  8.07578012]]\n"
     ]
    }
   ],
   "source": [
    "#labels = np.random.random_integers(low=0,high=1,size=2)\n",
    "labels = [0,1]\n",
    "print logistic_regression(y,x,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## features - NxD\n",
    "#  coefficients - DxK\n",
    "#  lables - Nx1\n",
    "def logistic_regression_inclusive_bias(features,coefficients,labels,step_size=0.001,max_iter=1000,lamda=0.1):\n",
    "    coeff_dim = np.shape(coefficients)\n",
    "    features_dim = np.shape(features)\n",
    "    bias = np.ones((coeff_dim[1],1))\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        ## softmax probabilities - shape (KxN)\n",
    "        probs = softmax(features,coefficients,bias,include_bias=1)\n",
    "\n",
    "        ## backpropagation\n",
    "        probs[labels,range(features_dim[0])] = probs[labels,range(features_dim[0])] - 1\n",
    "        #probs /= features_dim[0] \n",
    "        coefficients = coefficients - step_size*(np.dot(features.T,probs.T) + 2*lamda*coefficients)\n",
    "    \n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here the inputs coefficients and features should satisfy the dimensionality matching criteria \n",
    "def compute_accuracy(coefficients,features,output_labels):\n",
    "    ''' coefficients = (num_features,num_classes)\n",
    "        features = (num_inputs,num_features)\n",
    "        prediction_dot_product = (num_inputs,num_classes)\n",
    "        Thus, we have to find the index of the class with maximum score in axis=0\n",
    "        '''\n",
    "    features_dim = np.shape(features)\n",
    "    prediction = np.argmax(np.dot(features,model),axis=1)\n",
    "    accuracy = (prediction==output_labels)\n",
    "    return np.sum(accuracy,dtype='float32')*100/features_dim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.65654745  2.33628263  2.00515509  2.1891586 ]\n",
      " [ 4.96060011  4.16082751  4.38997593  4.50031274]\n",
      " [ 6.56211608  7.31682259  6.66433085  6.47430491]]\n"
     ]
    }
   ],
   "source": [
    "print logistic_regression_inclusive_bias(y,x,labels,lamda=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multinomial Regression over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"G:\\Miscellaneous\\python\\Utilities\\Datasets\\mnist.pkl\",'rb') as mnist_file:\n",
    "    train_data,valid_data,test_data = cPickle.load(mnist_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB4Ii/EBQhB8IivADQRF+IKj/AW7BpCpfXgfRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xad7ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## In this block we are shuffling the data to avoid any classification bias\n",
    "train_vec_in,train_label_out = train_data\n",
    "permutation = np.random.permutation(train_vec_in.shape[0])\n",
    "train_vec_in = train_vec_in[permutation,:]\n",
    "train_reshape = np.reshape(train_vec_in,newshape=[50000,28,28])\n",
    "train_label_out = train_label_out[permutation]\n",
    "\n",
    "plt.imshow(train_reshape[3],cmap='gray')\n",
    "print train_label_out[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_features,valid_labels = valid_data\n",
    "test_features,test_labels = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_scaling(features):\n",
    "    std = np.std(features,axis=0)\n",
    "    ind = np.where(std == 0)\n",
    "    std[ind] = 1\n",
    "    return (features-np.mean(features,axis=0))/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = set(train_label_out)\n",
    "step_size = 1e-4\n",
    "coefficients = np.zeros(shape=(np.shape(train_vec_in)[1],len(classes)))\n",
    "model = logistic_regression_inclusive_bias(train_vec_in,coefficients,train_label_out,step_size,max_iter=100,lamda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Set is 90.426%\n",
      "Accuracy on Test Set is 91.04%\n",
      "Accuracy on Validation Set is 91.3%\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy on Training Set is {}%\".format(compute_accuracy(model,train_vec_in,train_label_out))\n",
    "print \"Accuracy on Test Set is {}%\".format(compute_accuracy(model,test_features,test_labels))\n",
    "print \"Accuracy on Validation Set is {}%\".format(compute_accuracy(model,valid_features,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000L, 784L)\n",
      "(10000L,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(test_features)\n",
    "print np.shape(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Layer Neural Network using ReLUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(predicted_outcome,labels):\n",
    "    classes = set(labels)\n",
    "    num_classes = len(classes)\n",
    "    conf_mat = np.zeros((num_classes,num_classes))\n",
    "    for i in classes:\n",
    "        for j in classes:\n",
    "            conf_mat[i,j] = len(np.where((labels == i) & (predicted_outcome == j))[0])\n",
    "    \n",
    "    false_positives = np.sum(conf_mat,axis=0)\n",
    "    false_negatives = np.sum(conf_mat,axis=1)\n",
    "    precision = np.array([conf_mat[i,i]/false_positives[i] for i in range(10)])\n",
    "    recall = np.array([conf_mat[i,i]/false_negatives[i] for i in range(10)])\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_nn(features,labels,valid_features,valid_labels,hidden_sz=100,lamda=0.001,step_size=1,max_iter=100):\n",
    "    features_dim= np.shape(features)\n",
    "    classes = set(labels)\n",
    "    coeff_1 = 0.01 * np.random.randn(features_dim[1],hidden_sz)/(np.sqrt(features_dim[1]))\n",
    "    coeff_2 = 0.01 * np.random.randn(hidden_sz,len(classes))/(np.sqrt(hidden_sz))\n",
    "\n",
    "    coeff1_dim = np.shape(coeff_1)    \n",
    "    coeff2_dim = np.shape(coeff_2)\n",
    "    bias_1 = 0.01 * np.random.randn(1,coeff1_dim[1])\n",
    "    bias_2 = 0.01 * np.random.randn(1,coeff2_dim[1])\n",
    "    #bias_1 = np.zeros((1,coeff1_dim[1]))\n",
    "    #bias_2 = np.zeros((1,coeff2_dim[1]))\n",
    "\n",
    "    tot_cost = []\n",
    "    \n",
    "    print 'Training model for lamda:%.4f, and step_size: %.4f' % (lamda,step_size)\n",
    "    \n",
    "    for _ in xrange(max_iter):\n",
    "        #### Layer - 1 ####\n",
    "        reLU = np.maximum(0,np.dot(features,coeff_1) + bias_1)   # reLU - shape (Nxh)\n",
    "\n",
    "        ## without equalization ##\n",
    "        #l2_scores = np.dot(reLU,coeff_2) + bias_2  # [NxK]\n",
    "        #exp_scores = np.exp(l2_scores)\n",
    "        #probs_scores = exp_scores/np.sum(exp_scores,axis=1,keepdims=True)  # probs - shape(NxK)\n",
    "\n",
    "        ## with max-equalization ##\n",
    "        l2_scores = np.transpose(np.dot(reLU,coeff_2) + bias_2) # [KxN]\n",
    "        l2_scores_max = np.max(l2_scores,axis=0) # [1xN]\n",
    "        exp_scores = np.exp(l2_scores-l2_scores_max)\n",
    "        probs_scores = exp_scores/np.sum(exp_scores,axis=0,keepdims=True)  # probs - shape(KxN)\n",
    "\n",
    "        ## Loss computation\n",
    "        tot_cost.append(-np.sum(probs_scores[labels,range(features_dim[0])])/features_dim[0])\n",
    "        \n",
    "        # backpropagation\n",
    "        probs = np.transpose(probs_scores) # shape - NxK\n",
    "        probs[range(features_dim[0]),labels] -= 1\n",
    "        probs /= features_dim[0]\n",
    "\n",
    "        dcoeff_2 = np.dot(reLU.T,probs)   # dcoeff_2 - shape(hxK)\n",
    "        dbias_2 = np.sum(probs, axis=0, keepdims=True)\n",
    "        dhidden_layer = np.dot(probs,coeff_2.T)   # dhidden - shape(Nxh)\n",
    "        dhidden_layer[reLU <= 0] = 0\n",
    "        dcoeff_1 = np.dot(features.T,dhidden_layer)\n",
    "        dbias_1 = np.sum(dhidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "        coeff_2 += -step_size*(dcoeff_2 + lamda*coeff_2)  # coeff_2 - shape(hxK)\n",
    "        coeff_1 += -step_size*(dcoeff_1 + lamda*coeff_1)  # coeff_1 - shape(Dxh)\n",
    "        bias_1 += -step_size * dbias_1\n",
    "        bias_2 += -step_size * dbias_2\n",
    "        \n",
    "    ##### Training Data accuracy #####\n",
    "    hidden_layer_train = np.maximum(0,np.dot(features,coeff_1)+bias_1)\n",
    "    scores_train = np.dot(hidden_layer_train,coeff_2)+bias_2\n",
    "    predicted_class_train = np.argmax(scores_train, axis=1)\n",
    "    print 'training accuracy: %.2f' % (np.mean(predicted_class_train == labels))\n",
    "    #precision_train,recall_train = confusion_matrix(predicted_class_train,labels)\n",
    "    #print 'training mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_train),np.mean(recall_train))\n",
    "\n",
    "    ##### Validation Data accuracy #####\n",
    "    hidden_layer_valid = np.maximum(0,np.dot(valid_features,coeff_1)+bias_1)\n",
    "    scores_valid = np.dot(hidden_layer_valid,coeff_2)+bias_2\n",
    "    predicted_class_valid = np.argmax(scores_valid,axis=1)\n",
    "    print 'validation accuracy: %.2f' % (np.mean(predicted_class_valid == valid_labels))\n",
    "    #precision_valid,recall_valid = confusion_matrix(predicted_class_valid,valid_labels)\n",
    "    #print 'validation mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_valid),np.mean(recall_valid))   \n",
    "        \n",
    "    return coeff_1,bias_1,coeff_2,bias_2,tot_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e-04   5.99484250e-04   3.59381366e-03   2.15443469e-02\n",
      "   1.29154967e-01   7.74263683e-01   4.64158883e+00   2.78255940e+01\n",
      "   1.66810054e+02   1.00000000e+03]\n"
     ]
    }
   ],
   "source": [
    "print np.logspace(-4,3,num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for lamda:0.0001, and step_size: 1.0000\n",
      "training accuracy: 0.91\n",
      "training mean precision: 0.9105, mean recall: 0.9048\n",
      "validation accuracy: 0.92\n",
      "validation mean precision: 0.9222, mean recall: 0.9177\n",
      "Training model for lamda:0.0002, and step_size: 1.0000\n",
      "training accuracy: 0.91\n",
      "training mean precision: 0.9104, mean recall: 0.9053\n",
      "validation accuracy: 0.92\n",
      "validation mean precision: 0.9199, mean recall: 0.9165\n",
      "Training model for lamda:0.0005, and step_size: 1.0000\n",
      "training accuracy: 0.92\n",
      "training mean precision: 0.9213, mean recall: 0.9208\n",
      "validation accuracy: 0.93\n",
      "validation mean precision: 0.9282, mean recall: 0.9275\n",
      "Training model for lamda:0.0010, and step_size: 1.0000\n",
      "training accuracy: 0.92\n",
      "training mean precision: 0.9216, mean recall: 0.9215\n",
      "validation accuracy: 0.93\n",
      "validation mean precision: 0.9299, mean recall: 0.9294\n"
     ]
    }
   ],
   "source": [
    "''' Trained the model using the Grid Search Technique for Lamda.\n",
    "Given the randomness in weight initialization - lamda should be given in range (0.0001,0.001)'''\n",
    "#lamda = np.logspace(-4,0,num=10)\n",
    "lamda = np.logspace(-4,-3,num=4)\n",
    "for elem in lamda:\n",
    "    coeff_1,bias_1,coeff_2,bias_2 = two_layer_nn(features,labels,valid_features,valid_labels, \\\n",
    "                                             hidden_sz=100,lamda=elem,step_size=1,max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for lamda:0.0005, and step_size: 0.1000\n",
      "training accuracy: 0.77\n",
      "training mean precision: 0.7779, mean recall: 0.7670\n",
      "validation accuracy: 0.80\n",
      "validation mean precision: 0.8016, mean recall: 0.7903\n",
      "Training model for lamda:0.0005, and step_size: 0.5000\n",
      "training accuracy: 0.90\n",
      "training mean precision: 0.9016, mean recall: 0.9012\n",
      "validation accuracy: 0.91\n",
      "validation mean precision: 0.9096, mean recall: 0.9097\n",
      "Training model for lamda:0.0005, and step_size: 1.0000\n",
      "training accuracy: 0.90\n",
      "training mean precision: 0.9066, mean recall: 0.9030\n",
      "validation accuracy: 0.91\n",
      "validation mean precision: 0.9156, mean recall: 0.9133\n",
      "Training model for lamda:0.0005, and step_size: 1.5000\n",
      "training accuracy: 0.31\n",
      "training mean precision: nan, mean recall: 0.2962\n",
      "validation accuracy: 0.30\n",
      "validation mean precision: nan, mean recall: 0.3002\n",
      "Training model for lamda:0.0005, and step_size: 2.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:11: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.11\n",
      "training mean precision: nan, mean recall: 0.1001\n",
      "validation accuracy: 0.11\n",
      "validation mean precision: nan, mean recall: 0.1002\n",
      "Training model for lamda:0.0005, and step_size: 2.5000\n",
      "training accuracy: 0.11\n",
      "training mean precision: nan, mean recall: 0.1000\n",
      "validation accuracy: 0.11\n",
      "validation mean precision: nan, mean recall: 0.1000\n",
      "Training model for lamda:0.0005, and step_size: 3.0000\n",
      "training accuracy: 0.11\n",
      "training mean precision: nan, mean recall: 0.1000\n",
      "validation accuracy: 0.11\n",
      "validation mean precision: nan, mean recall: 0.1000\n",
      "Training model for lamda:0.0005, and step_size: 5.0000\n",
      "training accuracy: 0.12\n",
      "training mean precision: nan, mean recall: 0.1074\n",
      "validation accuracy: 0.11\n",
      "validation mean precision: nan, mean recall: 0.1083\n",
      "Training model for lamda:0.0005, and step_size: 7.5000\n",
      "training accuracy: 0.11\n",
      "training mean precision: nan, mean recall: 0.1000\n",
      "validation accuracy: 0.11\n",
      "validation mean precision: nan, mean recall: 0.1000\n",
      "Training model for lamda:0.0005, and step_size: 10.0000\n",
      "training accuracy: 0.11\n",
      "training mean precision: nan, mean recall: 0.1000\n",
      "validation accuracy: 0.11\n",
      "validation mean precision: nan, mean recall: 0.1000\n"
     ]
    }
   ],
   "source": [
    "## Here using lamda = 0.0005, we'll do a grid sweep for Step Size. \n",
    "# Reducing the step_size will require more number of iterations for optimization\n",
    "step_size = [0.1,0.5,1,1.5,2,2.5,3,5,7.5,10]\n",
    "for step in step_size:\n",
    "    coeff_1,bias_1,coeff_2,bias_2 = two_layer_nn(features,labels,valid_features,valid_labels, \\\n",
    "                                             hidden_sz=100,lamda=0.0005,step_size=step,max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for lamda:0.0010, and step_size: 1.0000\n",
      "training accuracy: 0.97\n",
      "validation accuracy: 0.96\n",
      "test accuracy: 0.96\n",
      "test mean precision: 0.9620, mean recall: 0.9615\n"
     ]
    }
   ],
   "source": [
    "coeff_1,bias_1,coeff_2,bias_2,tot_cost = two_layer_nn(train_vec_in,train_label_out,valid_features,valid_labels, \\\n",
    "                                             hidden_sz=100,lamda=0.001,step_size=1,max_iter=2000)\n",
    "\n",
    "##### Test Data accuracy #####\n",
    "hidden_layer_test = np.maximum(0,np.dot(test_features,coeff_1)+bias_1)\n",
    "scores_test = np.dot(hidden_layer_test,coeff_2)+bias_2\n",
    "predicted_class_test = np.argmax(scores_test,axis=1)\n",
    "print 'test accuracy: %.2f' % (np.mean(predicted_class_test == test_labels))\n",
    "precision_test,recall_test= confusion_matrix(predicted_class_test,test_labels)\n",
    "print 'test mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_test),np.mean(recall_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb7109b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tot_cost)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scaled = feature_scaling(train_vec_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = train_vec_in\n",
    "hidden_sz = 100\n",
    "classes = set(train_label_out)\n",
    "features_dim = np.shape(features)\n",
    "labels = train_label_out\n",
    "lamda = 0.001\n",
    "step_size = 1\n",
    "\n",
    "coeff_1 = 0.01 * np.random.randn(features_dim[1],hidden_sz)\n",
    "coeff_2 = 0.01 * np.random.randn(hidden_sz,len(classes))\n",
    "    \n",
    "coeff1_dim = np.shape(coeff_1)    \n",
    "coeff2_dim = np.shape(coeff_2)\n",
    "bias_1 = np.zeros((1,coeff1_dim[1]))\n",
    "bias_2 = np.zeros((1,coeff2_dim[1]))\n",
    "    \n",
    "for _ in xrange(100):\n",
    "    #### Layer - 1 ####\n",
    "    reLU = np.maximum(0,np.dot(features,coeff_1) + bias_1)   # reLU - shape (Nxh)\n",
    "    \n",
    "    ## without equalization ##\n",
    "    #l2_scores = np.dot(reLU,coeff_2) + bias_2  # [NxK]\n",
    "    #exp_scores = np.exp(l2_scores)\n",
    "    #probs_scores = exp_scores/np.sum(exp_scores,axis=1,keepdims=True)  # probs - shape(NxK)\n",
    "\n",
    "    ## with max-equalization ##\n",
    "    l2_scores = np.transpose(np.dot(reLU,coeff_2) + bias_2) # [KxN]\n",
    "    l2_scores_max = np.max(l2_scores,axis=0) # [1xN]\n",
    "    exp_scores = np.exp(l2_scores-l2_scores_max)\n",
    "    probs_scores = exp_scores/np.sum(exp_scores,axis=0,keepdims=True)  # probs - shape(KxN)\n",
    "    \n",
    "    # backpropagation\n",
    "    probs = np.transpose(probs_scores) # shape - NxK\n",
    "    probs[range(features_dim[0]),labels] -= 1\n",
    "    probs /= features_dim[0]\n",
    "\n",
    "    dcoeff_2 = np.dot(reLU.T,probs)   # dcoeff_2 - shape(hxK)\n",
    "    dbias_2 = np.sum(probs, axis=0, keepdims=True)\n",
    "    dhidden_layer = np.dot(probs,coeff_2.T)   # dhidden - shape(Nxh)\n",
    "    dhidden_layer[reLU <= 0] = 0\n",
    "    dcoeff_1 = np.dot(features.T,dhidden_layer)\n",
    "    dbias_1 = np.sum(dhidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "    coeff_2 += -step_size*(dcoeff_2 + lamda*coeff_2)  # coeff_2 - shape(hxK)\n",
    "    coeff_1 += -step_size*(dcoeff_1 + lamda*coeff_1)  # coeff_1 - shape(Dxh)\n",
    "    bias_1 += -step_size * dbias_1\n",
    "    bias_2 += -step_size * dbias_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10L, 50000L)\n",
      "(50000L,)\n",
      "(10L, 50000L)\n",
      "(10L, 50000L)\n",
      "(50000L, 10L)\n",
      "(1L, 50000L)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(l2_scores)\n",
    "print np.shape(l2_scores_max)\n",
    "print np.shape(exp_scores)\n",
    "print np.shape(probs_scores)\n",
    "print np.shape(probs)\n",
    "print np.shape(np.sum(exp_scores,axis=0,keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.90\n",
      "training mean precision: 0.9068, mean recall: 0.9028\n",
      "validation accuracy: 0.91\n",
      "validation mean precision: 0.9154, mean recall: 0.9114\n",
      "test accuracy: 0.91\n",
      "test mean precision: 0.9093, mean recall: 0.9055\n"
     ]
    }
   ],
   "source": [
    "##### Training Data accuracy #####\n",
    "hidden_layer_1 = np.maximum(0,np.dot(train_vec_in,coeff_1)+bias_1)\n",
    "scores_1 = np.dot(hidden_layer_1,coeff_2)+bias_2\n",
    "predicted_class_1 = np.argmax(scores_1, axis=1)\n",
    "print 'training accuracy: %.2f' % (np.mean(predicted_class_1 == train_label_out))\n",
    "precision_train,recall_train = confusion_matrix(predicted_class_1,train_label_out)\n",
    "print 'training mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_train),np.mean(recall_train))\n",
    "\n",
    "##### Validation Data accuracy #####\n",
    "hidden_layer_valid = np.maximum(0,np.dot(valid_features,coeff_1)+bias_1)\n",
    "scores_valid = np.dot(hidden_layer_valid,coeff_2)+bias_2\n",
    "predicted_class_valid = np.argmax(scores_valid,axis=1)\n",
    "print 'validation accuracy: %.2f' % (np.mean(predicted_class_valid == valid_labels))\n",
    "precision_valid,recall_valid = confusion_matrix(predicted_class_valid,valid_labels)\n",
    "print 'validation mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_valid),np.mean(recall_valid))\n",
    "\n",
    "##### Test Data accuracy #####\n",
    "hidden_layer_test = np.maximum(0,np.dot(test_features,coeff_1)+bias_1)\n",
    "scores_test = np.dot(hidden_layer_test,coeff_2)+bias_2\n",
    "predicted_class_test = np.argmax(scores_test,axis=1)\n",
    "print 'test accuracy: %.2f' % (np.mean(predicted_class_test == test_labels))\n",
    "precision_test,recall_test= confusion_matrix(predicted_class_test,test_labels)\n",
    "print 'test mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_test),np.mean(recall_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000L, 100L)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 2.302802\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters randomly\n",
    "D = 784\n",
    "K = 10\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-4 # regularization strength\n",
    "\n",
    "X = train_vec_in\n",
    "y = train_label_out\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in xrange(100):\n",
    "    # evaluate class scores, [N x K]\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 1000 == 0:\n",
    "        print \"iteration %d: loss %f\" % (i, loss)\n",
    "  \n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "  \n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    # finally into W,b\n",
    "    dW = np.dot(X.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "    # add regularization gradient contribution\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "  \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.92\n",
      "training mean precision: 0.9148, mean recall: 0.9137\n",
      "validation accuracy: 0.92\n",
      "validation mean precision: 0.9211, mean recall: 0.9202\n",
      "test accuracy: 0.92\n",
      "test mean precision: 0.9205, mean recall: 0.9195\n"
     ]
    }
   ],
   "source": [
    "##### Training Data accuracy #####\n",
    "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class_2 = np.argmax(scores, axis=1)\n",
    "print 'training accuracy: %.2f' % (np.mean(predicted_class_2 == y))\n",
    "precision_train_2,recall_train_2 = confusion_matrix(predicted_class_2,train_label_out)\n",
    "print 'training mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_train_2),np.mean(recall_train_2))\n",
    "\n",
    "##### Validation Data accuracy #####\n",
    "hidden_layer_valid_2 = np.maximum(0,np.dot(valid_features, W) + b)\n",
    "scores_valid_2 = np.dot(hidden_layer_valid_2,W2) + b2\n",
    "predicted_class_valid_2 = np.argmax(scores_valid_2,axis=1)\n",
    "print 'validation accuracy: %.2f' % (np.mean(predicted_class_valid_2 == valid_labels))\n",
    "precision_valid_2,recall_valid_2 = confusion_matrix(predicted_class_valid_2,valid_labels)\n",
    "print 'validation mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_valid_2),np.mean(recall_valid_2))\n",
    "\n",
    "##### Test Data accuracy #####\n",
    "hidden_layer_test_2 = np.maximum(0,np.dot(test_features, W) + b)\n",
    "scores_test_2 = np.dot(hidden_layer_test_2,W2) + b2\n",
    "predicted_class_test_2 = np.argmax(scores_test_2,axis=1)\n",
    "print 'test accuracy: %.2f' % (np.mean(predicted_class_test_2 == test_labels))\n",
    "precision_test_2,recall_test_2= confusion_matrix(predicted_class_test_2,test_labels)\n",
    "print 'test mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_test_2),np.mean(recall_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976418154316\n",
      "0.976314624215\n"
     ]
    }
   ],
   "source": [
    "precision,recall = confusion_matrix(predicted_class_1,train_label_out)\n",
    "print np.mean(precision)\n",
    "print np.mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9706\n"
     ]
    }
   ],
   "source": [
    "x = predicted_class_valid == valid_labels\n",
    "y = np.where(x == True)\n",
    "print len(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found submodule __check_build (is a package: True)\n",
      "Found submodule _build_utils (is a package: False)\n",
      "Found submodule _isotonic (is a package: False)\n",
      "Found submodule base (is a package: False)\n",
      "Found submodule calibration (is a package: False)\n",
      "Found submodule cluster (is a package: True)\n",
      "Found submodule covariance (is a package: True)\n",
      "Found submodule cross_decomposition (is a package: True)\n",
      "Found submodule cross_validation (is a package: False)\n",
      "Found submodule datasets (is a package: True)\n",
      "Found submodule decomposition (is a package: True)\n",
      "Found submodule discriminant_analysis (is a package: False)\n",
      "Found submodule dummy (is a package: False)\n",
      "Found submodule ensemble (is a package: True)\n",
      "Found submodule externals (is a package: True)\n",
      "Found submodule feature_extraction (is a package: True)\n",
      "Found submodule feature_selection (is a package: True)\n",
      "Found submodule gaussian_process (is a package: True)\n",
      "Found submodule grid_search (is a package: False)\n",
      "Found submodule isotonic (is a package: False)\n",
      "Found submodule kernel_approximation (is a package: False)\n",
      "Found submodule kernel_ridge (is a package: False)\n",
      "Found submodule lda (is a package: False)\n",
      "Found submodule learning_curve (is a package: False)\n",
      "Found submodule linear_model (is a package: True)\n",
      "Found submodule manifold (is a package: True)\n",
      "Found submodule metrics (is a package: True)\n",
      "Found submodule mixture (is a package: True)\n",
      "Found submodule multiclass (is a package: False)\n",
      "Found submodule naive_bayes (is a package: False)\n",
      "Found submodule neighbors (is a package: True)\n",
      "Found submodule neural_network (is a package: True)\n",
      "Found submodule pipeline (is a package: False)\n",
      "Found submodule preprocessing (is a package: True)\n",
      "Found submodule qda (is a package: False)\n",
      "Found submodule random_projection (is a package: False)\n",
      "Found submodule semi_supervised (is a package: True)\n",
      "Found submodule setup (is a package: False)\n",
      "Found submodule svm (is a package: True)\n",
      "Found submodule tests (is a package: True)\n",
      "Found submodule tree (is a package: True)\n",
      "Found submodule utils (is a package: True)\n"
     ]
    }
   ],
   "source": [
    "import pkgutil\n",
    "import sklearn\n",
    "package = sklearn\n",
    "\n",
    "for importer, modname, ispkg in pkgutil.iter_modules(package.__path__):\n",
    "    print \"Found submodule %s (is a package: %s)\" % (modname, ispkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "import graphlab as gl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"mnist.pkl\",'rb') as mnist_file:\n",
    "    train_data,valid_data,test_data = cPickle.load(mnist_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## In this block we are shuffling the data to avoid any classification bias\n",
    "train_vec_in,train_label_out = train_data\n",
    "permutation = np.random.permutation(train_vec_in.shape[0])\n",
    "train_vec_in = train_vec_in[permutation,:]\n",
    "train_reshape = np.reshape(train_vec_in,newshape=[50000,28,28])\n",
    "train_label_out = train_label_out[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_features,valid_labels = valid_data\n",
    "test_features,test_labels = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_probability(coefficients,features):\n",
    "    if np.shape(coefficients)[1] != np.shape(features)[0]:\n",
    "        coefficients = np.transpose(coefficients)\n",
    "    score = np.dot(coefficients,features)\n",
    "    return np.transpose(np.exp(score-np.amax(score,axis=0))/np.sum(np.exp(score-np.amax(score,axis=0)),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Coefficients = (num_classes,num_types_features)\n",
    "#  features = (num_datapoints,num_types_features)\n",
    "def softmax_regression(coefficients,features,output_label,classes,step_size,lamda,max_iter=1000,fit_intercept=True):\n",
    "    print np.shape(coefficients)\n",
    "    print np.shape(np.ones((len(classes),1)))\n",
    "    if fit_intercept:\n",
    "        features = np.concatenate((np.ones((np.shape(features)[0],1)),features),axis=1)\n",
    "        coefficients = np.concatenate((np.ones((len(classes),1)),coefficients),axis=1)\n",
    "    \n",
    "    features_dim = np.shape(features)\n",
    "    coeff_dim = np.shape(coefficients)\n",
    "    probs = np.zeros(shape=(coeff_dim[1],features_dim[0]))\n",
    "    cost = []\n",
    "    \n",
    "    for count in xrange(max_iter): \n",
    "        # Probability is of the shape = [number of datapoints, number of classes]\n",
    "        probs = softmax_probability(coefficients,features.T)\n",
    "        \n",
    "        # For l2 regularization we will not take the sum of intercept coefficients.\n",
    "        data_loss = -np.sum(np.log(probs[range(features_dim[0]),output_label])) + lamda*np.sum((coefficients*coefficients)[1:])\n",
    "        cost.append(data_loss) #/features_dim[0])\n",
    "\n",
    "        ''' Computing the gradient. \n",
    "        Shape of Features(X) = [Number of data points, number of features].\n",
    "        Shape of Probabilities = [number of data points, number of classes].\n",
    "        Here we will have to compute the outer product of X & Probabilities. Each column will then correspond to \n",
    "        the cost derivative w.r.t to corresponding class'''\n",
    "        probs[range(features_dim[0]),output_label] -= 1\n",
    "        derivative = np.dot(features.T,probs) + 2*lamda*coefficients.T\n",
    "        \n",
    "        ## Leaving out intercept from regularization\n",
    "        coefficients[:,0] = coefficients[:,0] - step_size*(derivative[0,:])\n",
    "        coefficients[:,1:] = coefficients[:,1:] - step_size*(derivative.T[:,1:] + 2*lamda*coefficients[:,1:])\n",
    "    return coefficients,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = set(train_label_out)\n",
    "step_size = 1e-5\n",
    "coefficients = np.zeros(shape=(len(classes),np.shape(train_vec_in)[1]))\n",
    "model_coefficients,cost = softmax_regression(coefficients,train_vec_in,train_label_out,classes,step_size,lamda=0, \\\n",
    "                                             max_iter=100,fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Accuracy on Training Set is {}%\".format(compute_accuracy(model_coefficients,train_vec_in.T,train_label_out))\n",
    "print \"Accuracy on Test Set is {}%\".format(compute_accuracy(model_coefficients,test_features.T,test_labels))\n",
    "print \"Accuracy on Validation Set is {}%\".format(compute_accuracy(model_coefficients,valid_features.T,valid_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

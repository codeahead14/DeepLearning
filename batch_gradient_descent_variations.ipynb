{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent Variations\n",
    "This Notebook provides scratch implementations of various batch gradient descent variations over MNIST Dataset.\n",
    "Before jumping to tools such as Tensorflow and Theano, getting a gist of how various strategies perform is really\n",
    "important and this notebook allows the user to run several experiments on a small neural network with one hidden layer.\n",
    "\n",
    "A simple two layer neural network has been utilised in these implementations.\n",
    "\n",
    "1. Stochastic Gradient Descent\n",
    "2. Stochastic Gradient Descent with Momentum\n",
    "\n",
    "Coming Soon\n",
    "1. Stochastic Gradient Descent with Nesterov Momentum\n",
    "2. Accelerated Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Datasets\\mnist.pkl\",'rb') as mnist_file:\n",
    "    train_data,valid_data,test_data = cPickle.load(mnist_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwg8k9X+apHgMoc41SAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6b7ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## In this block we are shuffling the data to avoid any classification bias\n",
    "train_vec_in,train_label_out = train_data\n",
    "permutation = np.random.permutation(train_vec_in.shape[0])\n",
    "train_vec_in = train_vec_in[permutation,:]\n",
    "train_reshape = np.reshape(train_vec_in,newshape=[50000,28,28])\n",
    "train_label_out = train_label_out[permutation]\n",
    "\n",
    "plt.imshow(train_reshape[3],cmap='gray')\n",
    "print train_label_out[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_features,valid_labels = valid_data\n",
    "test_features,test_labels = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(predicted_outcome,labels):\n",
    "    classes = set(labels)\n",
    "    num_classes = len(classes)\n",
    "    conf_mat = np.zeros((num_classes,num_classes))\n",
    "    for i in classes:\n",
    "        for j in classes:\n",
    "            conf_mat[i,j] = len(np.where((labels == i) & (predicted_outcome == j))[0])\n",
    "    \n",
    "    false_positives = np.sum(conf_mat,axis=0)\n",
    "    false_negatives = np.sum(conf_mat,axis=1)\n",
    "    precision = np.array([conf_mat[i,i]/false_positives[i] for i in range(10)])\n",
    "    recall = np.array([conf_mat[i,i]/false_negatives[i] for i in range(10)])\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smoothing(x,win_sz=30):\n",
    "    smoothing_window = np.ones(win_sz)/win_sz\n",
    "    smooth_func = np.convolve(x,smoothing_window)\n",
    "    return smooth_func[:-win_sz]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-layer NN with SGD over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_nn_sgd(features,labels,valid_features,valid_labels,hidden_sz=100, \\\n",
    "                     batch_size=200,epoch=10,lamda=0.001,step_size=1,max_iter=100):\n",
    "    features_dim= np.shape(features)\n",
    "    classes = set(labels)\n",
    "    coeff_1 = 0.01 * np.random.randn(features_dim[1],hidden_sz)/(np.sqrt(features_dim[1]))\n",
    "    coeff_2 = 0.01 * np.random.randn(hidden_sz,len(classes))/(np.sqrt(hidden_sz))\n",
    "\n",
    "    tot_cost,train_accuracy,valid_accuracy = [],[],[]\n",
    "    coeff1_dim = np.shape(coeff_1)    \n",
    "    coeff2_dim = np.shape(coeff_2)\n",
    "    bias_1 = 0.01 * np.random.randn(1,coeff1_dim[1])\n",
    "    bias_2 = 0.01 * np.random.randn(1,coeff2_dim[1])\n",
    "\n",
    "    model = {}\n",
    "\n",
    "    print 'Training model for batch_size:%d, lamda:%.4f, and step_size: %.4f' % (batch_size,lamda,step_size)\n",
    "    num_batches = features_dim[0]/batch_size\n",
    "    \n",
    "    for _ in xrange(epoch):\n",
    "        for i in xrange(num_batches):\n",
    "            batch_features = features[i*batch_size:(i+1)*batch_size]\n",
    "            batch_dim = np.shape(batch_features)\n",
    "            batch_labels = labels[i*batch_size:(i+1)*batch_size]\n",
    "            batch_labels_dim = np.shape(batch_labels)\n",
    "            \n",
    "            #### Layer - 1 ####\n",
    "            reLU = np.maximum(0,np.dot(batch_features,coeff_1) + bias_1)   # reLU - shape (Nxh)\n",
    "\n",
    "            ## with max-equalization ##\n",
    "            l2_scores = np.transpose(np.dot(reLU,coeff_2) + bias_2) # [KxN]\n",
    "            l2_scores_max = np.max(l2_scores,axis=0) # [1xN]\n",
    "            exp_scores = np.exp(l2_scores-l2_scores_max)\n",
    "            probs_scores = exp_scores/np.sum(exp_scores,axis=0,keepdims=True)  # probs - shape(KxN)\n",
    "            \n",
    "            ## Loss computation\n",
    "            tot_cost.append(-np.sum(probs_scores[batch_labels,range(batch_dim[0])])/batch_dim[0])\n",
    "        \n",
    "            # backpropagation\n",
    "            probs = np.transpose(probs_scores) # shape - NxK\n",
    "            probs[range(batch_dim[0]),batch_labels] -= 1\n",
    "            probs /= batch_dim[0]\n",
    "\n",
    "            dcoeff_2 = np.dot(reLU.T,probs)   # dcoeff_2 - shape(hxK)\n",
    "            dbias_2 = np.sum(probs, axis=0, keepdims=True)\n",
    "            dhidden_layer = np.dot(probs,coeff_2.T)   # dhidden - shape(Nxh)\n",
    "            dhidden_layer[reLU <= 0] = 0\n",
    "            dcoeff_1 = np.dot(batch_features.T,dhidden_layer)\n",
    "            dbias_1 = np.sum(dhidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "            coeff_2 += -step_size*(dcoeff_2 + lamda*coeff_2)  # coeff_2 - shape(hxK)\n",
    "            coeff_1 += -step_size*(dcoeff_1 + lamda*coeff_1)  # coeff_1 - shape(Dxh)\n",
    "            bias_1 += -step_size * dbias_1\n",
    "            bias_2 += -step_size * dbias_2\n",
    "            \n",
    "            ## Runtime Accuracy - Training and Validation \n",
    "            hidden_layer_train = np.maximum(0,np.dot(features,coeff_1)+bias_1)\n",
    "            scores_train = np.dot(hidden_layer_train,coeff_2)+bias_2\n",
    "            predicted_class_train = np.argmax(scores_train, axis=1)\n",
    "            train_accuracy.append(np.mean(predicted_class_train == labels))\n",
    "        \n",
    "            hidden_layer_valid = np.maximum(0,np.dot(valid_features,coeff_1)+bias_1)\n",
    "            scores_valid = np.dot(hidden_layer_valid,coeff_2)+bias_2\n",
    "            predicted_class_valid = np.argmax(scores_valid,axis=1)\n",
    "            valid_accuracy.append(np.mean(predicted_class_valid == valid_labels))\n",
    "        \n",
    "    ##### Training Data accuracy #####\n",
    "    hidden_layer_train = np.maximum(0,np.dot(features,coeff_1)+bias_1)\n",
    "    scores_train = np.dot(hidden_layer_train,coeff_2)+bias_2\n",
    "    predicted_class_train = np.argmax(scores_train, axis=1)\n",
    "    print 'training accuracy: %.4f' % (np.mean(predicted_class_train == labels))\n",
    "    precision_train,recall_train = confusion_matrix(predicted_class_train,labels)\n",
    "    print 'training mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_train),np.mean(recall_train))\n",
    "\n",
    "    ##### Validation Data accuracy #####\n",
    "    hidden_layer_valid = np.maximum(0,np.dot(valid_features,coeff_1)+bias_1)\n",
    "    scores_valid = np.dot(hidden_layer_valid,coeff_2)+bias_2\n",
    "    predicted_class_valid = np.argmax(scores_valid,axis=1)\n",
    "    print 'validation accuracy: %.4f' % (np.mean(predicted_class_valid == valid_labels))\n",
    "    precision_valid,recall_valid = confusion_matrix(predicted_class_valid,valid_labels)\n",
    "    print 'validation mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_valid),np.mean(recall_valid))   \n",
    "    \n",
    "    model['coeffs_l1'] = coeff_1\n",
    "    model['bias_l1'] = bias_1\n",
    "    model['coeffs_l2'] = coeff_2\n",
    "    model['bias_l2'] = bias_2 \n",
    "    model['train_accuracy'] = train_accuracy\n",
    "    model['valid_accuracy'] = valid_accuracy\n",
    "    model['tot_cost'] = tot_cost   \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for batch_size:50, lamda:0.0001, and step_size: 0.7500\n",
      "training accuracy: 0.9851\n",
      "training mean precision: 0.9852, mean recall: 0.9850\n",
      "validation accuracy: 0.9690\n",
      "validation mean precision: 0.9692, mean recall: 0.9689\n",
      "test accuracy: 0.9693\n",
      "test mean precision: 0.9694, mean recall: 0.9692\n"
     ]
    }
   ],
   "source": [
    "model_sgd = two_layer_nn_sgd(train_vec_in,train_label_out, \\\n",
    "                                                valid_features,valid_labels,batch_size=50, \\\n",
    "                                           epoch=20,hidden_sz=100,lamda=0.0001,step_size=0.75,max_iter=1000)\n",
    "\n",
    "##### Test Data accuracy #####\n",
    "hidden_layer_test = np.maximum(0,np.dot(test_features,model_sgd['coeffs_l1'])+model_sgd['bias_l1'])\n",
    "scores_test = np.dot(hidden_layer_test,model_sgd['coeffs_l2'])+model_sgd['bias_l2']\n",
    "predicted_class_test = np.argmax(scores_test,axis=1)\n",
    "print 'test accuracy: %.4f' % (np.mean(predicted_class_test == test_labels))\n",
    "precision_test,recall_test= confusion_matrix(predicted_class_test,test_labels)\n",
    "print 'test mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_test),np.mean(recall_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_nn_sgd_momentum(features,labels,valid_features,valid_labels,hidden_sz=100, \\\n",
    "                     batch_size=200,epoch=10,lamda=0.001,step_size=1,max_iter=100):\n",
    "    features_dim= np.shape(features)\n",
    "    classes = set(labels)\n",
    "    coeff_1 = 0.01 * np.random.randn(features_dim[1],hidden_sz)/(np.sqrt(features_dim[1]))\n",
    "    coeff_2 = 0.01 * np.random.randn(hidden_sz,len(classes))/(np.sqrt(hidden_sz))\n",
    "    \n",
    "    ## Momentum update parameters\n",
    "    mu_list = [0.5,0.9,0.95,0.99]\n",
    "    v_1 = np.zeros((features_dim[1],hidden_sz))\n",
    "    v_2 = np.zeros((hidden_sz,len(classes)))\n",
    "\n",
    "    tot_cost,train_accuracy,valid_accuracy = [],[],[]\n",
    "    coeff1_dim = np.shape(coeff_1)    \n",
    "    coeff2_dim = np.shape(coeff_2)\n",
    "    bias_1 = 0.01 * np.random.randn(1,coeff1_dim[1])\n",
    "    bias_2 = 0.01 * np.random.randn(1,coeff2_dim[1])\n",
    "    model = {}\n",
    "    \n",
    "    print 'Training model for batch_size:%d, lamda:%.4f, and step_size: %.4f' % (batch_size,lamda,step_size)\n",
    "    num_batches = features_dim[0]/batch_size\n",
    "    \n",
    "    for _ in xrange(epoch):\n",
    "        for i in xrange(num_batches):\n",
    "            batch_features = features[i*batch_size:(i+1)*batch_size]\n",
    "            batch_dim = np.shape(batch_features)\n",
    "            batch_labels = labels[i*batch_size:(i+1)*batch_size]\n",
    "            batch_labels_dim = np.shape(batch_labels)\n",
    "            \n",
    "            #### Layer - 1 ####\n",
    "            reLU = np.maximum(0,np.dot(batch_features,coeff_1) + bias_1)   # reLU - shape (Nxh)\n",
    "\n",
    "            ## with max-equalization ##\n",
    "            l2_scores = np.transpose(np.dot(reLU,coeff_2) + bias_2) # [KxN]\n",
    "            l2_scores_max = np.max(l2_scores,axis=0) # [1xN]\n",
    "            exp_scores = np.exp(l2_scores-l2_scores_max)\n",
    "            probs_scores = exp_scores/np.sum(exp_scores,axis=0,keepdims=True)  # probs - shape(KxN)\n",
    "            \n",
    "            ## Loss computation\n",
    "            tot_cost.append(-np.sum(probs_scores[batch_labels,range(batch_dim[0])])/batch_dim[0])\n",
    "        \n",
    "            # backpropagation\n",
    "            probs = np.transpose(probs_scores) # shape - NxK\n",
    "            probs[range(batch_dim[0]),batch_labels] -= 1\n",
    "            probs /= batch_dim[0]\n",
    "\n",
    "            dcoeff_2 = np.dot(reLU.T,probs)   # dcoeff_2 - shape(hxK)\n",
    "            dbias_2 = np.sum(probs, axis=0, keepdims=True)\n",
    "            dhidden_layer = np.dot(probs,coeff_2.T)   # dhidden - shape(Nxh)\n",
    "            dhidden_layer[reLU <= 0] = 0\n",
    "            dcoeff_1 = np.dot(batch_features.T,dhidden_layer)\n",
    "            dbias_1 = np.sum(dhidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "            v_2 = mu_list[0]*v_2 - (step_size*(dcoeff_2 + lamda*coeff_2))\n",
    "            coeff_2 += v_2    \n",
    "            bias_2 += -step_size * dbias_2\n",
    "            \n",
    "            v_1 = mu_list[0]*v_1 - (step_size*(dcoeff_1 + lamda*coeff_1))\n",
    "            coeff_1 += v_1\n",
    "            bias_1 += -step_size * dbias_1\n",
    "            \n",
    "            hidden_layer_train = np.maximum(0,np.dot(features,coeff_1)+bias_1)\n",
    "            scores_train = np.dot(hidden_layer_train,coeff_2)+bias_2\n",
    "            predicted_class_train = np.argmax(scores_train, axis=1)\n",
    "            train_accuracy.append(np.mean(predicted_class_train == labels))\n",
    "        \n",
    "            hidden_layer_valid = np.maximum(0,np.dot(valid_features,coeff_1)+bias_1)\n",
    "            scores_valid = np.dot(hidden_layer_valid,coeff_2)+bias_2\n",
    "            predicted_class_valid = np.argmax(scores_valid,axis=1)\n",
    "            valid_accuracy.append(np.mean(predicted_class_valid == valid_labels))\n",
    "        \n",
    "    ##### Training Data accuracy #####\n",
    "    hidden_layer_train = np.maximum(0,np.dot(features,coeff_1)+bias_1)\n",
    "    scores_train = np.dot(hidden_layer_train,coeff_2)+bias_2\n",
    "    predicted_class_train = np.argmax(scores_train, axis=1)\n",
    "    print 'training accuracy: %.4f' % (np.mean(predicted_class_train == labels))\n",
    "    precision_train,recall_train = confusion_matrix(predicted_class_train,labels)\n",
    "    print 'training mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_train),np.mean(recall_train))\n",
    "\n",
    "    ##### Validation Data accuracy #####\n",
    "    hidden_layer_valid = np.maximum(0,np.dot(valid_features,coeff_1)+bias_1)\n",
    "    scores_valid = np.dot(hidden_layer_valid,coeff_2)+bias_2\n",
    "    predicted_class_valid = np.argmax(scores_valid,axis=1)\n",
    "    print 'validation accuracy: %.4f' % (np.mean(predicted_class_valid == valid_labels))\n",
    "    precision_valid,recall_valid = confusion_matrix(predicted_class_valid,valid_labels)\n",
    "    print 'validation mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_valid),np.mean(recall_valid))   \n",
    "    \n",
    "    model['coeffs_l1'] = coeff_1\n",
    "    model['bias_l1'] = bias_1\n",
    "    model['coeffs_l2'] = coeff_2\n",
    "    model['bias_l2'] = bias_2 \n",
    "    model['train_accuracy'] = train_accuracy\n",
    "    model['tot_cost'] = tot_cost    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for batch_size:200, lamda:0.0001, and step_size: 0.7500\n",
      "training accuracy: 0.9903\n",
      "training mean precision: 0.9905, mean recall: 0.9901\n",
      "validation accuracy: 0.9702\n",
      "validation mean precision: 0.9707, mean recall: 0.9699\n",
      "test accuracy: 0.9710\n",
      "test mean precision: 0.9714, mean recall: 0.9707\n"
     ]
    }
   ],
   "source": [
    "model_sgd_momentum = two_layer_nn_sgd_momentum(train_vec_in,train_label_out,\n",
    "                          valid_features,valid_labels,epoch=20,hidden_sz=100,lamda=0.0001,step_size=0.75,max_iter=1000)\n",
    "\n",
    "##### Test Data accuracy #####\n",
    "hidden_layer_test = np.maximum(0,np.dot(test_features,model_sgd_momentum['coeffs_l1'])+model_sgd_momentum['bias_l1'])\n",
    "scores_test = np.dot(hidden_layer_test,model_sgd_momentum['coeffs_l2'])+model_sgd_momentum['bias_l2']\n",
    "predicted_class_test = np.argmax(scores_test,axis=1)\n",
    "print 'test accuracy: %.4f' % (np.mean(predicted_class_test == test_labels))\n",
    "precision_test,recall_test= confusion_matrix(predicted_class_test,test_labels)\n",
    "print 'test mean precision: %.4f, mean recall: %.4f' % (np.mean(precision_test),np.mean(recall_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nElK4i4gkIYW7iEgSUriLiCSh/we/HLnaq3jYzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x33b9e0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plots for Training accuracy for both the models\n",
    "\n",
    "plt.plot(smoothing(model_sgd['train_accuracy'])[0:250],'r')\n",
    "plt.plot(smoothing(model_sgd_momentum['train_accuracy'])[0:250],'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
